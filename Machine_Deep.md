  # 머신러닝과 딥러닝의 주요 개념들 정리
  <br/>
  <br/>
  
  ## 머신러닝

  주어진 데이터에서 x와 y의 관계를 w와 b를 이용하여 식을 세우는 일을 가설이라고 한다. 문제에 대한 규칙을 가장 잘 표현하는 w와 b를 찾는 것이 중요하다. 머신 러닝은 w와 b를 찾기 위해서 실제값과 가설로부터 얻은 예측값의 오차를 계산하는 식을 세우고, 이 식의 값을 최소화하는 최적의 w와 b를 찾아낸다.
  <br/>
  <br/>

  ### - 비용 함수(Cost function)
   <br/>
  실제값과 예측값에 대한 오차에 대한 식을 목적 함수(Objective function) 또는 비용 함수(Cost function) 또는 손실 함수(Loss function) 라고 한다. 함수의 값을 최소화하거나, 최대화하거나 하는 목적을 가진 함수를 목적 함수(Objective function), 값을 최소화하려고 하면 이를 비용 함수(Cost function) 또는 손실 함수(Loss function)라고 한다.
  <br/>
  <br/>

  ### - 옵티마이저(Optimizer)
 <br/>
  선형 회귀를 포함한 수많은 머신 러닝, 딥 러닝의 학습은 결국 비용 함수를 최소화하는 매개 변수인 w와 b을 찾기 위한 작업을 수행한다. 이때 사용되는 알고리즘을 옵티마이저(Optimizer) 또는 최적화 알고리즘이라고 부른다.

  그리고 이 옵티마이저를 통해 적절한 w와 b를 찾아내는 과정을 머신 러닝에서 훈련(training) 또는 학습(learning)이라고 부른다. 
  <br/>
  <br/>

  ### -  ex) 선형 회귀
  - 선형회귀의 비용 함수(Cost function) : 평균 제곱 오차(MSE)
  - 선형회귀의 옵티마이저(Optimizer) : 경사하강법(Gradient Descent)
  <br/>
  <br/>

  ### -  기계학습 전 데이터의 분리 <br/><br/>

  머신 러닝을 위한 데이터를 준비했다면 기계를 학습하기 전 해당 데이터를 훈련용(Training), 검증용(Validation), 테스트용(Testing) 이렇게 세 가지로 분리하는 것이 일반적이다.  
  - 훈련 데이터는 머신 러닝 모델을 학습하는 용도이다. 테스트 데이터는 학습한 머신 러닝 모델의 성능을 평가하기 위한 용도이다.  
  - 검증용 데이터는 모델의 성능을 평가하기 위한 용도가 아니라 모델의 성능을 조정하기 위한 용도이다. 더 정확히는 모델이 훈련 데이터에 과적합(overfitting)이 되고 있는지 판단하거나 하이퍼파라미터의 조정을 위한 용도이다.    
  - 훈련용 데이터로 훈련을 모두 시킨 모델은 검증용 데이터를 사용하여 정확도를 검증하며 하이퍼파라미터를 튜닝(tuning) 한다. 검증용 데이터에 대해서 높은 정확도를 얻도록 하이퍼파라미터의 값을 바꿔보는 것이다.
  <br/>
  <br/>

  ### -  하이퍼파라미터와 파라미터(매개변수)
<br>

  - 하이퍼파라미터(초매개변수) : 모델의 성능에 영향을 주는 사람이 값을 지정하는 변수. 경사 하강법에서의 학습률(learning rate)이나, 딥 러닝에서 뉴런의 수나 층의 수와 같은 것들이 대표적인 하이퍼파라미터이다.  

  - 매개변수 : 가중치와 편향. 학습을 하는 동안 값이 계속해서 변하는 수. 가중치와 편향과 같은 매개변수는 사용자가 결정해주는 값이 아니라 모델이 학습하는 과정에서 얻어지는 값이다.
  <br/>
  <br/>

  ### -  분류(Classification)와 회귀(Regression)
   <br/>

  1) 이진 분류 문제(Binary Classification)
  이진 분류는 주어진 입력에 대해서 두 개의 선택지 중 하나의 답을 선택해야 하는 것. 종합 시험 성적표를 보고 최종적으로 합격, 불합격인지 판단하는 문제, 메일을 보고나서 정상 메일, 스팸 메일인지를 판단하는 문제 등이 이에 속한다.<br/><br/>

  2) 다중 클래스 분류(Multi-class Classification)
  다중 클래스 분류는 주어진 입력에 대해서 세 개 이상의 선택지 중에서 답을 선택해야 하는 것. 예를 들어 서점 직원이 일을 하는데 과학, 영어, IT, 학습지, 만화라는 레이블이 붙어있는 5개의 책장이 있다고 하고 새 책이 입고되면, 이 책은 다섯 개의 책장 중에서 분야에 맞는 적절한 책장에 책을 넣어야 한다. 이 경우는 현실에서의 다중 클래스 분류 문제라고 할 수 있다.<br/><br/>

  3) 회귀 문제(Regression)
  회귀 문제는 분류 문제처럼 둘 중 하나를 선택해야 한다거나, 책이 입고되었을 때 5개의 책장 중 하나의 책장을 골라야하는 경우처럼 정답이 몇 개의 정해진 선택지 중에서 정해져 있는 경우가 아니라 어떠한 연속적인 값의 범위 내에서 예측값이 나오는 경우를 말한다.  
              
      예를 들어 역과의 거리, 인구 밀도, 방의 개수 등을 입력하면 부동산 가격을 예측하는 머신 러닝 모델이 있다고 하면 머신 러닝 모델이 부동산 가격을 7억 8,456만 3,450원으로 예측하는 경우도 있을 것이고, 8억 1257만 300원으로 예측하는 경우도 있을 수 있다. 즉, 특정 값의 범위 내에서는 어떤 숫자도 나올 수 있다. 기존의 분류 문제와 같이 분리된(비연속적인) 답이 결과가 아니라 연속된 값을 결과로 가지는 이러한 문제를 회귀 문제라고 부른다. 회귀 문제의 예시로 시계열 데이터(Time Series Data)를 이용한 주가 예측, 생산량 예측, 지수 예측 등이 있다.
  <br/>
  <br/>

  ### -  원-핫 인코딩(One-Hot Encoding)  
 <br/>
  컴퓨터는 문자보다는 숫자를 더 잘 처리한다. 이를 위해 자연어 처리에서는 문자를 숫자로 바꾸는 여러가지 기법들이 있다. 원-핫 인코딩은 그 기법들 중에서 단어를 표현하는 가장 기본적인 표현 방법이다.   
  원-핫 인코딩은 단어 집합의 크기를 벡터의 차원으로 하고, 표현하고 싶은 단어의 인덱스에 1의 값을 부여하고, 다른 인덱스에는 0을 부여하는 단어의 벡터 표현 방식이다. 이렇게 표현된 벡터를 원-핫 벡터(One-Hot vector)라고 한다.
  <br/>
  <br/>

  ### -  소프트맥스 회귀(Softmax Regression)
 <br/>
  이진 분류가 두 개의 선택지 중 하나를 고르는 문제라면, 다중 클래스 분류는 세 개 이상의 선택지 중 하나를 고르는 문제이다. 로지스틱 회귀를 통해 2개의 선택지 중에서 1개를 고르는 이진 분류(Binary Classification)문제를 푼다면 3개 이상의 선택지 중에서 1개를 고르는 다중 클래스 분류 문제는 소프트맥스 회귀를 통해 풀 수 있다.    

 <br/>
  소프트맥스 함수는 선택해야 하는 선택지의 총 개수를 k라고 할 때, k차원의 벡터를 입력받아 각 클래스에 대한 확률을 추정한다.   
  다수의 클래스를 분류하는 문제에서는 이진 분류처럼 2개의 숫자 레이블이 아니라 클래스의 개수만큼 숫자 레이블이 필요하다. 이때 생각해볼 수 있는 레이블링 방법은 분류해야 할 클래스 전체에 정수 인코딩을 하는 것이다. 예를 들어서 분류해야 할 레이블이 {red, green, blue}와 같이 3개라면 각각 0, 1, 2로 레이블한다. 또는 분류해야 할 클래스가 4개고 인덱스를 숫자 1부터 시작하고 싶다면 {baby, child, adolescent, adult}라면 1, 2, 3, 4로 레이블을 해볼 수 있다. 
  
  <br/>
  그런데 일반적인 다중 클래스 분류 문제에서 레이블링 방법으로는 위와 같은 정수 인코딩이 아니라 원-핫 인코딩을 사용하는 것이 보다 클래스의 성질을 잘 표현하였다고 할 수 있다. 그 이유는 정수 인코딩과 달리 원-핫 인코딩은 분류 문제 모든 클래스 간의 관계를 균등하게 분배하기 때문이다. 물론 각 클래스가 순서의 의미를 갖고 있어 회귀를 통해 분류 문제를 풀 수 있는 경우는 정수 인코딩의 순서 정보가 도움이 된다.
  <br/>
  <br/>
   <br/>

  ### -  혼동 행렬(Confusion Matrix)
   <br/>

  머신 러닝에서는 맞춘 문제수를 전체 문제수로 나눈 값을 정확도(Accuracy)라고 한다. 하지만 정확도는 맞춘 결과와 틀린 결과에 대한 세부적인 내용을 알려주지는 않는다. 이를 위해서 사용하는 것이 혼동 행렬(Confusion Matrix)이다. 
   <br/>
  | -   | 예측 참   | 예측 거짓 |
  |:--------|:--------:|--------:|
  | 실제 참   | TP | FN       |
  | 실제 거짓     | FP   | TN      |
   <br/>
  * True Positive(TP) : 실제 True인 정답을 True라고 예측 (정답)
  * False Positive(FP) : 실제 False인 정답을 True라고 예측 (오답)
  * False Negative(FN) : 실제 True인 정답을 False라고 예측 (오답)
  * True Negative(TN) : 실제 False인 정답을 False라고 예측 (정답)
  <br/>
  <br/>

  ### -  훈련 데이터의 오차와 테스트 데이터의 오차는 손실이라고도 부른다.
  <br/>
  <br/>

  ### -  과적합 방지를 고려한 일반적인 딥 러닝 모델의 학습 과정

  1. 주어진 데이터를 훈련 데이터, 검증 데이터, 테스트 데이터로 나눈다. 가령, 6:2:2 비율로 나눌 수 있다.
  2. 훈련 데이터로 모델을 학습한다. (에포크 +1)
  3. 검증 데이터로 모델을 평가하여 검증 데이터에 대한 정확도와 오차(loss)를 계산한다.
  4. 검증 데이터의 오차가 증가하였다면 과적합 징후이므로 학습 종료 후 Step 5로 이동, 아니라면 Step 2.로 재이동한다.
  5. 모델의 학습이 종료되었으니 테스트 데이터로 모델을 평가한다.
  <br/>
  <br/>
    

  ### -  손실 함수(Loss function)
   <br/>
  손실 함수는 실제값과 예측값의 차이를 수치화해주는 함수이다. 이 두 값의 차이. 즉, 오차가 클 수록 손실 함수의 값은 크고 오차가 작을 수록 손실 함수의 값은 작아진다. 회귀에서는 평균 제곱 오차, 분류 문제에서는 크로스 엔트로피를 주로 손실 함수로 사용한다. 손실 함수의 값을 최소화하는 두 개의 매개변수인 가중치 w와 편향 b의 값을 찾는 것이 딥 러닝의 학습 과정이므로 손실 함수의 선정은 매우 중요하다.  
    <br/>
    
  1) 평균 제곱 오차, MSE(Mean Squared Error, MSE): 선형 회귀, 즉 연속형 변수를 예측할 때 사용.
  2) 이진 크로스 엔트로피(Binary Cross-Entropy): 이항 교차 엔트로피라고도 부르는 손실 함수. 출력층에서 시그모이드 함수를 사용하는 이진 분류, 로지스틱 회귀에서 사용.
  3) 카테고리칼 크로스 엔트로피(Categorical Cross-Entropy): 출력층에서 소프트맥스 함수를 사용하는 다중 클래스 분류(Multi-Class Classification)일 경우 사용.
  <br/>
  <br/>
  
  ### -  배치 크기(Batch Size)에 따른 경사 하강법
   <br/>
  손실 함수의 값을 줄여나가면서 학습하는 방법은 어떤 옵티마이저를 사용하느냐에 따라 달라진다. 여기서 배치(Batch)라는 개념에 대한 이해가 필요한데, 배치는 가중치 등의 매개 변수의 값을 조정하기 위해 사용하는 데이터의 양을 말한다. 전체 데이터를 가지고 매개 변수의 값을 조정할 수도 있고, 정해준 양의 데이터만 가지고도 매개 변수의 값을 조정할 수 있다.  

  1) 배치 경사 하강법(Batch Gradient Descent): 가장 기본적인 경사 하강법. 배치 경사 하강법은 옵티마이저 중 하나로 오차(loss)를 구할 때 전체 데이터를 고려. 딥 러닝에서는 전체 데이터에 대한 한 번의 훈련 횟수를 1 에포크라고 하는데, 배치 경사 하강법은 한 번의 에포크에 모든 매개변수 업데이트를 단 한 번 수행. 전체 데이터를 고려해서 학습하므로 한 번의 매개 변수 업데이트에 시간이 오래 걸리며, 메모리를 크게 요구한다는 단점이 있다.  
  
  2) 배치 크기가 1인 확률적 경사 하강법(Stochastic Gradient Descent, SGD): 매개변수 값을 조정 시 전체 데이터가 아니라 랜덤으로 선택한 하나의 데이터에 대해서만 계산하는 방법. 더 적은 데이터를 사용하므로 더 빠르게 계산 가능. 확률적 경사 하강법은 매개변수의 변경폭이 불안정하고, 배치 경사 하강법보다 정확도가 낮을 수도 있지만 하나의 데이터에 대해서만 메모리에 저장하면 되므로 자원이 적은 컴퓨터에서도 쉽게 사용 가능.  
  
  3) 미니 배치 경사 하강법(Mini-Batch Gradient Descent): 가장 많이 사용하는 경사 하강법. 전체 데이터도, 1개의 데이터도 아닐 때, 배치 크기를 지정하여 해당 데이터 개수만큼에 대해서 계산하여 매개 변수의 값을 조정하는 경사 하강법. 전체 데이터를 계산하는 것보다 빠르며, SGD보다 안정적이라는 장점이 있다. 배치 크기는 일반적으로 2의 n제곱에 해당하는 숫자로 선택하는 것이 보편적. 만약, model.fit()에서 배치 크기를 별도로 지정해주지 않을 경우에 기본값은 2의 5제곱에 해당하는 숫자인 32로 설정된다.
  <br/>
  <br/>
  
  ### -  에포크와 배치 크기와 이터레이션(Epochs and Batch size and Iteration)
   <br/>

   > <br/>사람마다 동일한 문제지와 정답지를 주더라도 공부 방법은 사실 천차만별입니다. 어떤 사람은 문제지 하나를 다 풀고 나서 정답을 채점하는데 어떤 사람은 문제지의 문제를 10개 단위로 끊어서 공부합니다. 문제 10개를 풀고 채점하고 다시 다음 문제 10개를 풀고 채점하고 반복하는 방식으로 학습하는 방식입니다. 또한 게으른 사람은 문제지를 세 번 공부하는데, 성실한 사람은 문제지의 문제를 달달 외울만큼 문제지를 100번 공부합니다. 기계도 똑같습니다. 같은 문제지와 정답지를 주더라도 공부 방법을 다르게 설정할 수 있습니다.<br/>
   <br/>
    
  <br/>

  #### 1) 에포크(Epoch)
   <br/>
  에포크란 인공 신경망에서 전체 데이터에 대해서 순전파와 역전파가 끝난 상태를 말한다. 전체 데이터를 하나의 문제지에 비유한다면 문제지의 모든 문제를 끝까지 다 풀고, 정답지로 채점을 하여 문제지에 대한 공부를 한 번 끝낸 상태이다.

  만약 에포크가 50이라고 하면, 전체 데이터 단위로는 총 50번 학습한다. 문제지에 비유하면 문제지를 50번 푼 셈이다. 이 에포크 횟수가 지나치거나 너무 적으면 과적합이나 과소적합이 발생할 수 있다.
   <br/>
 #### 2) 배치 크기(Batch size)
   <br/>
  배치 크기는 몇 개의 데이터 단위로 매개변수를 업데이트 하는지를 말한다. 현실에 비유하면 문제지에서 몇 개씩 문제를 풀고나서 정답지를 확인하느냐의 문제이다. 사람은 문제를 풀고 정답을 보는 순간 부족했던 점을 깨달으며 지식이 업데이트 된다면 기계 입장에서는 실제값과 예측값으로부터 오차를 계산하고 옵티마이저가 매개변수를 업데이트한다. 중요한 포인트는 업데이트가 시작되는 시점이 정답지/실제값을 확인하는 시점이라는 것이다.
  
  사람이 2,000 문제가 수록되어있는 문제지의 문제를 200개 단위로 풀고 채점한다고 하면 이때 배치 크기는 200이다. 기계는 배치 크기가 200이면 200개의 샘플 단위로 가중치를 업데이트 한다.  

  배치 크기와 배치의 수는 다른 개념이다. 전체 데이터가 2,000일때 배치 크기를 200으로 준다면 배치의 수는 10이다. 이는 에포크에서 배치 크기를 나눠준 값(2,000/200 = 10)이기도 하다. 이때 배치의 수를 이터레이션이라고 한다.  
    <br/>
  #### 3) 이터레이션(Iteration) 또는 스텝(Step)
  <br/>
  이터레이션이란 한 번의 에포크를 끝내기 위해서 필요한 배치의 수를 말한다. 또는 한 번의 에포크 내에서 이루어지는 매개변수의 업데이트 횟수이기도 하다. 전체 데이터가 2,000일 때 배치 크기를 200으로 한다면 이터레이션의 수는 총 10이다. 이는 한 번의 에포크 당 매개변수 업데이트가 10번 이루어진다는 것을 의미한다. 배치 크기가 1인 확률적 경사 하강법을 이 개념을 가지고 다시 설명하면 배치 크기가 1이므로 모든 이터레이션마다 하나의 데이터를 선택하여 경사 하강법을 수행한다. 이터레이션은 스텝(Step)이라고 부르기도 한다. 
  <br/>
  <br/>
  
  ### -  훈련(training) 또는 학습(learning)
   <br/>
  머신 러닝은 데이터가 주어지면, 기계가 스스로 데이터로부터 규칙성을 찾는 것에 집중한다. 이렇게 주어진 데이터로부터 규칙성을 찾는 과정을 훈련(training) 또는 학습(learning)이라고 한다.
  <br/>
  <br/>

  
  ### -  훈련(training) 또는 학습(learning)
   <br/>
  머신 러닝은 데이터가 주어지면, 기계가 스스로 데이터로부터 규칙성을 찾는 것에 집중한다. 이렇게 주어진 데이터로부터 규칙성을 찾는 과정을 훈련(training) 또는 학습(learning)이라고 한다.
  <br/>
  <br/>
